# Your are a MASTER Python Developer capble of accurate and concise code analysis and 'bug' correction.
# I will provide the specifications of the computer in use.
# I will provide you with the current code I am working with.
# I will provide the specifications of the computer in use.
# You will assess the code, the associated '#comments', and the error message.
# You will identify the specific block or blocks of code, based on the provide code block labeling, i.e. 'BLOCK FOUR' or 'BLOCK EIGHT', that is/are responsible for the error.
# You will provide in a code window the identified block or blocks of code with appropriate corrections
#



#

#

#

#

#

#

#









#

# Imports
import sys
import os
import platform
from os.path import exists
from watermark import watermark

import torch
import pandas as pd
from sklearn.model_selection import train_test_split

#


#

# Set the device
device = "mps" if torch.backends.mps.is_available() else "cpu"
print(f"Using device: {device}")


# Report Technologies
print(f'Python Platform: {platform.platform()}')
print(f'Python {sys.version}')
print(watermark())
print(watermark(iversions=True, globals_=globals()))

# Check PyTorch has access to MPS (Metal Performance Shader, Apple's GPU architecture)
print(f"PyTorch version: {torch.__version__}")
print(f"Is MPS (Metal Performance Shader) built? {torch.backends.mps.is_built()}")
print(f"Is MPS available? {torch.backends.mps.is_available()}")


# Using wikipedia BTC page edit history as proxy for public interest and sentiment
# - additional potential sources for 'sentiment analysis' => tweets, google trends
"""
    Downloading wikipedia edits for BTC  """


#
# BLOCK ONE
#

import mwclient  # Module for interacting with MediaWiki API.
import time  # Module for handling time-related operations.

# Initialize a MediaWiki client site which is a class that enables work with a specific wiki site,
# in this case, the English (en) version of Wikipedia.
site = mwclient.Site('en.wikipedia.org')  # Replace 'en' with the appropriate language code for other sites.

# Specify which page to use by creating a Page object using the page's title.
page = site.pages['Bitcoin']  # Replace 'Bitcoin' with the title of alternatively desired Wikipedia page.

#


#
# BLOCK TWO
#

# Obtain from Wikipedia the list of revisions for the specified page.
revs = list(page.revisions())  # Returns a list of dictionaries containing information about each revision of the page.
                              # Each dictionary contains keys such as 'user', 'comment', 'timestamp', etc. that provide
                              # details about the revision. The list is sorted in reverse chronological order by default.
#


#
# BLOCK
#

# Look at the first set of revisions for the specified page.
# Outputs an ordered dictionary, which is a Python class that functions as a hybrid-like list and dictionary combined.
revs[0]  # Returns a dictionary containing information about the first revision of the page.
         # The dictionary contains keys such as 'user', 'comment', 'timestamp', etc. that provide details about the revision.
         # The specific information returned depends on the MediaWiki API and the parameters used to query the revisions.

#


#
# BLOCK THREE
#

# Sort the list of revisions for the specified page in ascending order based on their timestamps.
# The sorted() function is used here with a key parameter, which takes a function that returns a value to sort by.
# In this case, the lambda function returns the timestamp value from each revision dictionary.
revs = sorted(revs, key=lambda rev: rev['timestamp'])  # Returns the same list of revision dictionaries, but sorted
                                                       # in ascending order by their timestamps.
    
#


# Look at the first revision in the sorted/reordered list of revisions for the specified page.
# Outputs an ordered dictionary, which is a Python class that functions as a hybrid-like list and dictionary combined.
revs[0]  
# Returns a dictionary containing information about the first revision of the page in the sorted list.


#
# BLOCK FOUR
#

# import the regular expressions module
import re

# modified pattern string
fixed_pattern = "(.*?)-\\d{5}-of-\\d{5}"

# create a regular expression object using the fixed pattern string
re_obj = re.compile(fixed_pattern)

# use the regular expression object to search for matches in a string
text = "This is a test string-12345-of-67890."
match = re_obj.search(text)

print(match.group(0))

#


#
# BLOCK FIVE
#

# This library provides for pipelines to 'deep learning' models; requires TensorFlow 2.0 or PyTorch
# AutoModelForSequenceClassification provides a pre-trained sequence classification model
# AutoTokenizer is used to automatically select the appropriate tokenizer for a given pre-trained model. 
# The tokenizer is responsible for preprocessing raw text data into a format understood by the deep learning model.
# The pipeline function is used for running pre-trained models.
from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline


# Load the pre-trained model and tokenizer
model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")

# Create a sentiment analysis pipeline using the loaded model and tokenizer
sentiment_pipeline = pipeline('sentiment-analysis',
                              model=model,            # use the loaded model
                              tokenizer=tokenizer,    # use the tokenizer
                              framework='pt',         # set the deep learning framework to PyTorch
                              max_length=256,         # set the maximum input length to a smaller value (e.g., 256)
                              truncation=True)

#


#
# BLOCK SIX
#

from typing import List, Dict, Any
# from transformers import pipeline

def find_sentiment(texts: List[str]) -> List[float]:
    """
    This function takes in a list of strings of text and returns a list of sentiment scores between -1 and 1,
    where negative values indicate negative sentiment and positive values indicate positive sentiment.
    """
    # Load the sentiment analysis model
    # sentiment_pipeline = pipeline("sentiment-analysis")
    
    scores: List[float] = []
    for text in texts:
        # Run a list of texts through the sentiment analysis model and get a list of sentiment scores, 
        # each score is a dictionary containing the sentiment label ('POSITIVE' or 'NEGATIVE') and score (a float between 0 and 1)
        sents: List[Dict[str, Any]] = sentiment_pipeline([text[:250]])  # run the text through the sentiment analysis model and get the result
        
        # Extract the sentiment score from the dictionary
        score = sents[0]['score']
        if sents[0]['label'] == 'NEGATIVE':
            score = -score
        
        # Append the sentiment score to the list of scores
        scores.append(score)
        
    return scores

#


#
# BLOCK SEVEN
#

# Test the sentiment analysis model
# Define a list of text strings to analyze
texts = [
    "This is a positive sentence.",
    "This is a negative sentence.",
    "This is a neutral sentence.",
]

# Call the find_sentiment function to analyze the texts
sentiment_scores = find_sentiment(texts)

# Print the sentiment scores
print(sentiment_scores)

#

#


#
# BLOCK EIGHT
#

# Define the 'edits' dictionary
edits: Dict[str, Dict[str, Any]] = {}

# Iterate through the revisions in the sorted list 'revs'
for rev in revs:
    # Get the date of the revision using the 'timestamp' attribute
    date: str = time.strftime('%Y-%m-%d', rev['timestamp'])
    
    # Check if the date is not in the 'edits' dictionary
    if date not in edits:
        # If the date is not in the 'edits' dictionary, create a new dictionary with the following attributes:
        # - 'sentiments': a list to store the sentiment analysis scores
        # - 'edt_count': an integer to store the number of edits made on the date
        edits[date]: Dict[str, Any] = {'sentiments': [], 'edt_count': 0}
        
    # Increment the 'edt_count' by 1 for the current date
    edits[date]['edt_count'] += 1
        
    # Get the comment for the revision, if it exists
    comment: str = rev.get('comment', '')
    
    # Call the 'find_sentiment' function to get the sentiment score for the comment
    sentiment_score: float = find_sentiment(comment)
    
    # Append the sentiment score to the 'sentiments' list for the current date
    edits[date]['sentiments'].append(sentiment_score)

#


#
# BLOCK NINE
#

from statistics import mean  # import mean function from statistics module

# iterate through each key in the edits dictionary
for key in edits:
    
    # check if there are any sentiments present for the current key
    if len(edits[key]['sentiments']) > 0:
        
        # calculate the mean sentiment for the current key
        edits[key]['sentiment'] = mean(edits[key]['sentiments'])
        
        # calculate the negative sentiment score as a ratio of the total sentiment scores
        edits[key]['percent_neg_sentiment'] = len([s for s in edits[key]['sentiments'] if s < 0]) / len(edits[key]['sentiments'])
                                           
    # if there are no sentiments present for the current key
    else:
        
        # set the sentiment score to 0
        edits[key]['sentiment'] = 0
                                           
        # set the negative sentiment score to 0
        edits[key]['neg_sentiment'] = 0

        # remove the 'sentiments' key from the current key in the 'edits' dictionary
        del edits[key]['sentiments']


#
# BLOCK SIX
#

# Provides for pipeline to 'deep learning' models; requires TensorFlow 2.0 or PyTorch
# from transformers import pipeline  # import the transformers library, which includes the pipeline function for running pre-trained models

# initialize the Deep Learning sentiment analysis model using the pipeline function from transformers
# reduced maximum input length to a smaller value (e.g., 256)
# switch to PyTorch by setting the framework parameter to "pt"
# sentiment_pipeline = pipeline("sentiment-analysis",
#                               model="distilbert-base-uncased-finetuned-sst-2-english",
#                               tokenizer="distilbert-base-uncased",
#                               framework="pt")


# sentiment_pipeline = pipeline("sentiment-analysis",
#                               model="distilbert-base-uncased-finetuned-sst-2-english",
#                               tokenizer="distilbert-base-uncased",
#                               framework="pt",
#                               max_length=256)

#


# from transformers import AutoModelForSequenceClassification, AutoTokenizer

# # Set the model and tokenizer names
# model_name = "distilbert-base-uncased-finetuned-sst-2-english"
# tokenizer_name = "distilbert-base-uncased"

# # Load the pre-trained model and tokenizer
# model = AutoModelForSequenceClassification.from_pretrained(model_name)
# tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)

# # Create a sentiment analysis pipeline using the loaded model and tokenizer
# sentiment_pipeline = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)





#
# BLOCK SEVEN
#

# from typing import Dict, List, Tuple, Any  # import List and Tuple types from the typing module for type hinting

# def find_sentiment(text: List[str]) -> List[float]:
#     """
#     This function takes in a list of strings of text and returns a list of sentiment scores between -1 and 1,
#     where negative values indicate negative sentiment and positive values indicate positive sentiment.
#     """
    
#     # Run a list of texts through the sentiment analysis model and get a list of sentiment scores, 
#     # each score is a dictionary containing the sentiment label ('POSITIVE' or 'NEGATIVE') and score (a float between 0 and 1)

#     sents: List[Dict[str, Any]] = sentiment_pipeline([text[:250]])[0]  # run the text through the sentiment analysis model and get the result
#     scores: List[float] = []
#     for sent in sents:
#         score: float = sent['score']
#         if sent['label'] == 'NEGATIVE':
#             score *= -1
#         scores.append(score)
#     return scores

#
































#

#

#

#


#BLOCK ALPHA
# Define the 'edits' dictionary
edits: Dict[str, Dict[str, Any]] = {}

# Iterate through the revisions in the sorted list 'revs'
for rev in revs:
    # Get the date of the revision using the 'timestamp' attribute
    date: str = time.strftime('%Y-%m-%d', rev['timestamp'])
    
    # Check if the date is not in the 'edits' dictionary
    if date not in edits:
        # If the date is not in the 'edits' dictionary, create a new dictionary with the following attributes:
        # - 'sentiments': a list to store the sentiment analysis scores
        # - 'edt_count': an integer to store the number of edits made on the date
        edits[date]: Dict[str, Any] = {'sentiments': [], 'edt_count': 0, 'sentiment': 0, 'percent_neg_sentiment': 0}
        
    # Increment the 'edt_count' by 1 for the current date
    edits[date]['edt_count'] += 1
        
    # Get the comment for the revision, if it exists
    comment: str = rev.get('comment', '')
    
    # Call the 'find_sentiment' function to get the sentiment score for the comment
    sentiment_score: float = find_sentiment(comment)
    
    # Append the sentiment score to the 'sentiments' list for the current date
    edits[date]['sentiments'].append(sentiment_score)

#BLOCK BETA
from statistics import mean  # import mean function from statistics module


# iterate through each key in the edits dictionary
for key in edits:
    
    # calculate the mean sentiment for the current key
    if 'sentiments' in edits[key] and len(edits[key]['sentiments']) > 0:
        edits[key]['sentiment'] = mean(edits[key]['sentiments'])
        edits[key]['percent_neg_sentiment'] = len([s for s in edits[key]['sentiments'] if s < 0]) / len(edits[key]['sentiments'])
        
    # if there are no sentiments present for the current key
    else:
        edits[key]['sentiment'] = 0
        edits[key]['percent_neg_sentiment'] = 0
        
        # remove the 'sentiments' key from the current key in the 'edits' dictionary
        if 'sentiments' in edits[key]:
            del edits[key]['sentiments']

        
        
#


# Define the 'edits' dictionary
edits: Dict[str, Dict[str, Any]] = {}

# Iterate through the revisions in the sorted list 'revs'
for rev in revs:
    # Get the date of the revision using the 'timestamp' attribute
    date: str = time.strftime('%Y-%m-%d', rev['timestamp'])
    
    # Check if the date is not in the 'edits' dictionary
    if date not in edits:
        # If the date is not in the 'edits' dictionary, create a new dictionary with the following attributes:
        # - 'sentiments': a list to store the sentiment analysis scores
        # - 'edt_count': an integer to store the number of edits made on the date
        edits[date]: Dict[str, Any] = {'sentiments': [], 'edt_count': 0}
        
    # Increment the 'edt_count' by 1 for the current date
    edits[date]['edt_count'] += 1
        
    # Get the comment for the revision, if it exists
    comment: str = rev.get('comment', '')
    
    # Call the 'find_sentiment' function to get the sentiment score for the comment
    sentiment_score: float = find_sentiment(comment)
    
    # Append the sentiment score to the 'sentiments' list for the current date
    edits[date]['sentiments'].append(sentiment_score)



from statistics import mean  # import mean function from statistics module

# iterate through each key in the edits dictionary
for key in edits:
    
    # check if there are any sentiments present for the current key
    if len(edits[key]['sentiments']) > 0:
        
        # calculate the mean sentiment for the current key
        edits[key]['sentiment'] = mean(edits[key]['sentiments'])
        
        # calculate the negative sentiment score as a ratio of the total sentiment scores
        edits[key]['percent_neg_sentiment'] = len([s for s in edits[key]['sentiments'] if s < 0]) / len(edits[key]['sentiments'])
                                           
    # if there are no sentiments present for the current key
    else:
        
        # set the sentiment score to 0
        edits[key]['sentiment'] = 0
                                           
        # set the negative sentiment score to 0
        edits[key]['neg_sentiment'] = 0

        # remove the 'sentiments' key from the current key in the 'edits' dictionary
        del edits[key]['sentiments']



# 'edits' is Key value Pair/s dictionary, each key => date
# there are 3 key values; 'edt_count', 'sentiment', and 'neg_sentiment'
# 'edt_count' = number of times BTC wikipedia page comments was edited on a given day
# 'sentiment' = average sentiment for that day
# 'percent_neg_sentiment' = percentage of edits that express sentiment negativity on that given day

# edits


# Import pandas module for creating and manipulating dataframes
import pandas as pd

# Create a dataframe 'edits_df' from the dictionary 'edits' with index orientation set to 'index'
# takes in list of dictionaries, orient on 'index' ensures that each dictionary is a seperate row of the dataframe
BTC_edits_df = pd.DataFrame.from_dict(edits, orient='index')


BTC_edits_df.head()


from DateTime import DateTime

dates = pd.date_range(start= '2009-03-08', end= datetime.now())



