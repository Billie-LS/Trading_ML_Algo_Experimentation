{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "549364f0-d609-4e67-ad67-bcc8b976cec4",
   "metadata": {},
   "source": [
    "Using historic data\n",
    "using public sentiment - sentiment analysis\n",
    "using XGBoost model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aff40f7-2844-4545-b250-3e41788dab33",
   "metadata": {},
   "source": [
    "!pip install tensorflow-metal\n",
    "%env TF_METAL_ENABLED=1\n",
    "!pip install --upgrade tensorflow-metal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2eedd1b3-a483-46b6-8c23-6c0b4da8077b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "dlopen(/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow-plugins/libmetal_plugin.dylib, 0x0006): symbol not found in flat namespace '_TF_GetInputPropertiesList'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplatform\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m exists\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/__init__.py:439\u001b[0m\n\u001b[1;32m    437\u001b[0m _plugin_dir \u001b[38;5;241m=\u001b[39m _os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(_s, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow-plugins\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(_plugin_dir):\n\u001b[0;32m--> 439\u001b[0m   \u001b[43m_ll\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_plugin_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m   \u001b[38;5;66;03m# Load Pluggable Device Library\u001b[39;00m\n\u001b[1;32m    441\u001b[0m   _ll\u001b[38;5;241m.\u001b[39mload_pluggable_device_library(_plugin_dir)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/framework/load_library.py:151\u001b[0m, in \u001b[0;36mload_library\u001b[0;34m(library_location)\u001b[0m\n\u001b[1;32m    148\u001b[0m     kernel_libraries \u001b[38;5;241m=\u001b[39m [library_location]\n\u001b[1;32m    150\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m lib \u001b[38;5;129;01min\u001b[39;00m kernel_libraries:\n\u001b[0;32m--> 151\u001b[0m     \u001b[43mpy_tf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_LoadLibrary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlib\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    155\u001b[0m       errno\u001b[38;5;241m.\u001b[39mENOENT,\n\u001b[1;32m    156\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe file or folder to load kernel libraries from does not exist.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    157\u001b[0m       library_location)\n",
      "\u001b[0;31mNotFoundError\u001b[0m: dlopen(/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow-plugins/libmetal_plugin.dylib, 0x0006): symbol not found in flat namespace '_TF_GetInputPropertiesList'"
     ]
    }
   ],
   "source": [
    "from watermark import watermark\n",
    "# Imports\n",
    "import sys\n",
    "import os\n",
    "import platform\n",
    "from os.path import exists\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9cd819-3cfc-4627-b746-f6906c994730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report Technologies\n",
    "print(f'Python Platform: {platform.platform()}')\n",
    "print(f'Python {sys.version}')\n",
    "print(watermark())\n",
    "print(watermark(iversions=True, globals_=globals()))\n",
    "\n",
    "mps_available = tf.config.list_logical_devices(\"MPS\")\n",
    "device = \"/device:MPS:0\" if len(mps_available) > 0 else \"/device:CPU:0\"\n",
    "print(f\"TensorFlow version: {tf.__version__}\\nIs MPS available?: {len(mps_available) > 0}\\nUsing device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4ed731-956e-4db8-89f7-138cae2668df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using wikipedia BTC page edit history as proxy for public interest and sentiment\n",
    "# - additional potential sources for 'sentiment analysis' => tweets, google trends\n",
    "\"\"\"\n",
    "    Downloading wikipedia edits for BTC  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b66fe1-7b9c-4347-8393-15892ca8ac26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mwclient  # Module for interacting with MediaWiki API.\n",
    "import time  # Module for handling time-related operations.\n",
    "\n",
    "# Initialize a MediaWiki client site which is a class that enables work with a specific wiki site,\n",
    "# in this case, the English (en) version of Wikipedia.\n",
    "site = mwclient.Site('en.wikipedia.org')  # Replace 'en' with the appropriate language code for other sites.\n",
    "\n",
    "# Specify which page to use by creating a Page object using the page's title.\n",
    "page = site.pages['Bitcoin']  # Replace 'Bitcoin' with the title of alternatively desired Wikipedia page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b318930e-1e6b-4103-b68d-adabf3901352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain from wikipedia the list of revisions\n",
    "revs = list(page.revisions()) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602583b1-8f15-42d7-a010-21086d5dea79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain from Wikipedia the list of revisions for the specified page.\n",
    "revs = list(page.revisions())  # Returns a list of dictionaries containing information about each revision of the page.\n",
    "                              # Each dictionary contains keys such as 'user', 'comment', 'timestamp', etc. that provide\n",
    "                              # details about the revision. The list is sorted in reverse chronological order by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3274d97f-9012-432f-a7d4-d059b5f6851f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the first set of revisions for the specified page.\n",
    "# Outputs an ordered dictionary, which is a Python class that functions as a hybrid-like list and dictionary combined.\n",
    "revs[0]  # Returns a dictionary containing information about the first revision of the page.\n",
    "         # The dictionary contains keys such as 'user', 'comment', 'timestamp', etc. that provide details about the revision.\n",
    "         # The specific information returned depends on the MediaWiki API and the parameters used to query the revisions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7802a3c8-d1e2-40b9-a518-9f0bb5b375c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the list of revisions for the specified page in ascending order based on their timestamps.\n",
    "# The sorted() function is used here with a key parameter, which takes a function that returns a value to sort by.\n",
    "# In this case, the lambda function returns the timestamp value from each revision dictionary.\n",
    "revs = sorted(revs, key=lambda rev: rev['timestamp'])  # Returns the same list of revision dictionaries, but sorted\n",
    "                                                       # in ascending order by their timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dbabb2-2b85-4d9b-b46f-c0501aed74d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the first revision in the sorted/reordered list of revisions for the specified page.\n",
    "# Outputs an ordered dictionary, which is a Python class that functions as a hybrid-like list and dictionary combined.\n",
    "revs[0]  \n",
    "# Returns a dictionary containing information about the first revision of the page in the sorted list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69df3e90-ba03-4e12-922a-efd42d495c23",
   "metadata": {},
   "source": [
    "# Identifying the 'sentiment'of the wikipedia page edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5619cce7-5e87-4ecd-82d3-82b892c9da8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provides for pipeline to 'deep learning' models; requires TensorFlow 2.0 or PyTorch\n",
    "from transformers import pipeline  # import the transformers library, which includes the pipeline function for running pre-trained models\n",
    "\n",
    "from typing import List, Tuple  # import List and Tuple types from the typing module for type hinting\n",
    "\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")  # initialize the Deep Learning sentiment analysis model using the pipeline function from transformers\n",
    "\n",
    "def find_sentiment(text: str) -> float:\n",
    "    \"\"\"\n",
    "    This function takes in a string of text and returns a sentiment score between -1 and 1,\n",
    "    where negative values indicate negative sentiment and positive values indicate positive sentiment.\n",
    "    \"\"\"\n",
    "    sent: List[dict] = sentiment_pipeline([text[:259]])[0]  # run the text through the sentiment analysis model and get the first (and only) result\n",
    "    score: float = sent['score']  # extract the sentiment score from the model output\n",
    "    if sent['label'] == 'NEGATIVE':  # check if the sentiment label is negative\n",
    "        score *= -1  # if so, multiply the score by -1 to indicate negative sentiment\n",
    "    return score  # return the sentiment score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87be060c-d3d6-408d-9894-24d79ce66a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sentiment function operation\n",
    "test_positive_sent = find_sentiment('I love you') \n",
    "print(test_positive_sent)\n",
    "\n",
    "test_negative_sent = find_sentiment('I despise you') \n",
    "print(test_negative_sent)\n",
    "\n",
    "test_neutral_sent = find_sentiment('I neutral you') \n",
    "print(test_neutral_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb79240-efb9-4649-9873-57345fb9394a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Any\n",
    "import time\n",
    "\n",
    "# Define the 'edits' dictionary\n",
    "edits: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "# Iterate through the revisions in the sorted list 'revs'\n",
    "for rev in revs:\n",
    "    # Get the date of the revision using the 'timestamp' attribute\n",
    "    date: str = time.strftime('%Y-%m-%d', rev['timestamp'])\n",
    "    \n",
    "    # Check if the date is not in the 'edits' dictionary\n",
    "    if date not in edits:\n",
    "        # If the date is not in the 'edits' dictionary, create a new dictionary with the following attributes:\n",
    "        # - 'sentiments': a list to store the sentiment analysis scores\n",
    "        # - 'edt_count': an integer to store the number of edits made on the date\n",
    "        edits[date]: Dict[str, Any] = {'sentiments': [], 'edt_count': 0}\n",
    "        \n",
    "    # Increment the 'edt_count' by 1 for the current date\n",
    "    edits[date]['edt_count'] += 1\n",
    "        \n",
    "    # Get the comment for the revision\n",
    "    comment: str = rev['comment']\n",
    "    \n",
    "    # Call the 'find_sentiment' function to get the sentiment score for the comment\n",
    "    sentiment_score: float = find_sentiment(comment)\n",
    "    \n",
    "    # Append the sentiment score to the 'sentiments' list for the current date\n",
    "    edits[date]['sentiments'].append(sentiment_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0e65ae-8c69-40d1-a219-b2a82aa9f017",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean  # import mean function from statistics module\n",
    "\n",
    "# iterate through each key in the edits dictionary\n",
    "for key in edits:\n",
    "    \n",
    "    # check if there are any sentiments present for the current key\n",
    "    if len(edits[key]['sentiments']) > 0:\n",
    "        \n",
    "        # calculate the mean sentiment for the current key\n",
    "        edits[key]['sentiment'] = mean(edits[key]['sentiments'])\n",
    "        \n",
    "        # calculate the negative sentiment score as a ratio of the total sentiment scores\n",
    "        edits[key]['neg_sentiment'] = len([s for s in edits[key]['sentiments'] if s < 0]) / len(edits[key]['sentiments'])\n",
    "                                           \n",
    "    # if there are no sentiments present for the current key\n",
    "    else:\n",
    "        \n",
    "        # set the sentiment score to 0\n",
    "        edits[key]['sentiment'] = 0\n",
    "                                           \n",
    "        # set the negative sentiment score to 0\n",
    "        edits[key]['neg_sentiment'] = 0\n",
    "\n",
    "        # remove the 'sentiments' key from the current key in the 'edits' dictionary\n",
    "        del edits[key]['sentiments']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f41da4b-2f6b-4fdc-821e-eb3820223136",
   "metadata": {},
   "source": [
    "# generate a dataframe with sentiment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a82cf31-edb9-4775-9dcf-c9ae609409fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas module for creating and manipulating dataframes\n",
    "import pandas as pd\n",
    "\n",
    "# Create a dataframe 'edits_df' from the dictionary 'edits' with index orientation set to 'index'\n",
    "edits_df = pd.DataFrame.from_dict(edits, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cba34c-c31a-43f9-b6d0-9e4827e68437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfa9ae8-83d6-4efb-b937-802757988e7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b731f5-adf0-4214-b4fd-be571e491919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory data anlysis\n",
    "# Downloading assets/securities price data (i.e. S&P 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff841917-3d6b-4b4f-b967-d5650108944c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import yfinance package as package for data source\n",
    "import yfinance as yf  # call yahoo finance API in order to download daily stock and indices prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0223d321-1d60-4b25-ad2f-0e2cf6e9641c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a ticker class to enable download of pricing history for. single, i.e. specified symbol\n",
    "sp500 = yf.Ticker('^GSPC')  # GSPC is the S&P500 index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3606b87-471e-4156-8f82-7842fc6fc7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to query historical prices\n",
    "sp500 = sp500.history(period = 'max') # 'max' allows to query ALL data from beginning of index creation in dataframe format\n",
    "\n",
    "# visualize pandas dataframe\n",
    "# each row representing data of a trading day with non-trading days excluded\n",
    "sp500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb443a84-c341-4a5d-9d1d-9c4801053993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the dataframe index; note returns a 'date-time-index'\n",
    "sp500.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb40873-330e-4f50-bdd9-e32535e05f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate preliminary data cleaning and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd1645c-aa8e-4512-8975-f2702af9324c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize data frame data via plotting closing price column against the index\n",
    "sp500.plot.line(y = 'Close', use_index = True)  # index values are x-axis and closing price is y-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142ab74d-4512-4fd3-9d73-d435f3bedd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminate columns that provide zero value for our analysis of the S&P500 index\n",
    "del sp500['Dividends']\n",
    "del sp500['Stock Splits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d318a4-d8c1-4df6-a15c-5932d1371ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target for Machine Learning, i.e. what we will actually utilize Machine Leanring to predict\n",
    "# i.e. target / question is ~ will price go up or go down TOMORROW\n",
    "# note, predicting direction will be both more successful and more advantagious / greater utility than attempting to predict absolute price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307b0c93-efe7-4025-be17-8cc2dce76600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column in our dataframe to provide tomorrow's price\n",
    "# creating new dataframe column called 'Tomorrow' \n",
    "# use pandas function '.shift()' to move the values of the 'Close' column one row ahead and \n",
    "# assign them to the 'Tomorrow' column, creating new column with the previous day's closing price\n",
    "sp500['Tomorrow'] = sp500['Close'].shift(-1)  \n",
    "\n",
    "\n",
    "# Visualize updated 'sp500' dataframe with new 'Tomorrow' column containing previous day's closing price.\n",
    "# allowing visualization if the market has gone up or down.\n",
    "sp500 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7b2505-bda2-4e38-bf38-85eb222955ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizing 'Tomorrow's' price, we will define our 'Target' as a new column in the dataframe\n",
    "# creating a new column named 'Target' in the dataframe.\n",
    "# comparing the values of the 'Tomorrow' column to the values of the 'Close' column\n",
    "# This generates a boolean (True or False) value depending on whether the 'Tomorrow' price is greater than the 'Close' price. \n",
    "sp500['Target'] = (sp500['Tomorrow'] > sp500['Close'])  \n",
    "\n",
    "# using the pandas '.astype()' function to convert the 'Target' column boolean to integers. \n",
    "sp500['Target'] = (sp500['Tomorrow'] > sp500['Close']).astype(int)  \n",
    "\n",
    "# Visualize updated 'sp500' dataframe with the new 'Target' column values changed from True or False to integer 1 or 0\n",
    "# value of 1 means 'Tomorrow' price was greater than the 'Close' price\n",
    "# value of 0 means that the 'Tomorrow' price was less than or equal to the 'Close' price.\n",
    "sp500 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbb87ed-9734-4c44-b070-d3e12f92367c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and/or streamline data by specifying a smaller range, i.e. how far back we will go for our starting point\n",
    "# Use pandas '.loc' method to specify using only those rows that are at least the specified date or later,\n",
    "# Use pandas '.copy()' method method returns a copy of the DataFrame, meaning (by default) any changes made in the original DataFrame will NOT be reflected in the copy.\n",
    "\n",
    "sp500 = sp500.loc['1990-01-01':].copy() \n",
    "# Here, we are using the pandas '.loc' method to specify that we only want to keep rows in the 'sp500' dataframe \n",
    "# that have a date of January 1st, 1990 or later. \n",
    "# We are then using the '.copy()' method to make a copy of the resulting dataframe so that any changes made to the \n",
    "# copy will not affect the original dataframe. This ensures that we are working with a clean and streamlined dataset.\n",
    "\n",
    "# Visualize the dataframe with the defined starting row \n",
    "sp500 \n",
    "# Here, we are displaying the updated 'sp500' dataframe with the defined starting row of January 1st, 1990. \n",
    "# This allows us to focus on more recent data and remove any older data that may not be relevant to our analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597610ff-f16f-489b-aea5-f23fe406915a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# develop initial Machine Learning Model, i.e. training a 'random forest' model\n",
    "# random forest, by design tendency towards resistance to 'overfitting'\n",
    "# random forest generally relatively quick in operation\n",
    "# random forest can identify or recognize 'non-linear' relationships of the data which is good in modeling securities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a806b444-5965-4dac-9253-a1964bed3e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import our first model package\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcd6672-4027-44df-b01b-d15a5f3f545e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the model\n",
    "# Set the random state, 'seed' to a specific value to ensure the same results are reproducible in the future\n",
    "seed = 42 \n",
    "\n",
    "\n",
    "# Create a Random Forest Classifier model with specified hyperparameters:\n",
    "# n_estimators specify the number of random decision trees will use; generally accuracy improves with higher number of decision trees\n",
    "#    setting the number of decision trees to 185, which is generally a good number for accuracy\n",
    "# min_samples_split helps prevent overfitting by specifying the minimum number of samples required to split an internal node; the higher this value the LOWER the accuracy,\n",
    "#    setting minimum number of samples required to split an internal node to 100, which helps to prevent overfitting.\n",
    "# random_state is used to ensure the same random values are used for each run of the model for reproducibility\n",
    "model = RandomForestClassifier(n_estimators=185, min_samples_split = 100, random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996cec9e-5333-447b-9b6a-09a581c0314a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and testing datasets\n",
    "# need to split taking 'time series' nature into account, as random splitting will result in leakage, i.e. using tomorrow's data to predict today\n",
    "train = sp500.iloc[:-100]  # training data set - use all data rows EXCEPT for the most recent/ last 100 rows\n",
    "test =  sp500.iloc[-100:]  # testing data set - use the most recent/ last 100 rows\n",
    "\n",
    "# select the columns that will be used to train the model\n",
    "predictors = ['Open', 'High', 'Low', 'Close', 'Volume']  # exclude the 'Target' and 'Tomorrow' columns to prevent leakage!\n",
    "\n",
    "# train the model using the specified predictors and target variables\n",
    "model.fit(train[predictors], train['Target'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b887fe9e-31e4-41d0-b182-f33a4a9fd173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use trained model using the '.predict()' method passing in 'test' set with the predictors to make predictions on the testing dataset\n",
    "preds = model.predict(test[predictors])\n",
    "\n",
    "# visualize predictions (note, default will be in numpy array format) \n",
    "preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd95064-1571-4500-8fac-7ebab4f172b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# create new pandas series with predicted values using the 'preds' array and the index from the test dataset\n",
    "preds = pd.Series(preds, index = test.index)\n",
    "\n",
    "# visualize the predictions as a pandas series\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d3b599-0cf9-438e-8dd4-0077526dea50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assess the accuracy of the model - require metrics package/s\n",
    "from sklearn.metrics import (classification_report, \n",
    "                             balanced_accuracy_score,\n",
    "                             precision_score,\n",
    "                             confusion_matrix,\n",
    "                             roc_auc_score,\n",
    "                             f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2c73f4-dac7-42a3-b007-a91faf2b8f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the precision score using predicted values 'preds' and actual values in the test set 'test['Target']'\n",
    "precision_score_1 = precision_score(test['Target'], preds)\n",
    "\n",
    "# visualize precision score\n",
    "print(precision_score_1)\n",
    "\n",
    "# Calculating the balanced accuracy score using predicted values 'preds' and actual values in the test set 'test['Target']'\n",
    "balanced_accuracy_score_1 = balanced_accuracy_score(test['Target'], preds)\n",
    "\n",
    "# visualize balanced accuracy score\n",
    "print(balanced_accuracy_score_1)\n",
    "\n",
    "# Calculating the confusion matrix using predicted values 'preds' and actual values in the test set 'test['Target']'\n",
    "confusion_matrix_1 = confusion_matrix(test['Target'], preds)\n",
    "\n",
    "# visualize confusion matrix\n",
    "print(confusion_matrix_1)\n",
    "\n",
    "# Calculating the classification report using predicted values 'preds' and actual values in the test set 'test['Target']'\n",
    "classification_report_1 = classification_report(test['Target'], preds)\n",
    "\n",
    "# Printing the classification report\n",
    "print(classification_report_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8d1c8a-bd3a-41df-be51-02377f53b520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the actual target values and predicted values into a new dataframe using concatenation; i.e. pd.concat()\n",
    "combined = pd.concat([test['Target'], preds], axis=1)  \n",
    "\n",
    "# visualize the combined values dataframe\n",
    "print(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e58592-7eff-4c6a-9aad-4d33c290a4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the 'combined' dataframe using a line plot\n",
    "combined.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce96b386-82f1-4cdb-8f30-7ad34a9537b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14413bbe-edb2-474d-abaa-bc5952dc6468",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List\n",
    "\n",
    "# functionalize the prediction model\n",
    "def predict(train: pd.DataFrame, test: pd.DataFrame, predictors: List[str], model: Any) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generates predictions for a test set using a trained machine learning model.\n",
    "    \n",
    "    Args:\n",
    "        train: DataFrame of the training data, including the target variable.\n",
    "        test: DataFrame of the test data.\n",
    "        predictors: List of feature names to use for making predictions.\n",
    "        model: A trained machine learning model object with a '.fit()' and '.predict()' method.\n",
    "    \n",
    "    Returns:\n",
    "        A DataFrame containing the actual target values and predicted values for the test set.\n",
    "    \"\"\"\n",
    "    \n",
    "    # fit/'train' model using the specified training data and predictor variables\n",
    "    model.fit(train[predictors], train['Target'])\n",
    "    \n",
    "    # generate predictions using the fitted model with the '.predict()' method passing in 'test' set with the predictors\n",
    "    preds = model.predict(test[predictors])\n",
    "    \n",
    "    # convert the numpy array of predicted values to a pandas Series with the same index as the test data\n",
    "    # name the Series \"Predictions\"\n",
    "    preds = pd.Series(preds, index = test.index, name = 'Predictions') \n",
    "    \n",
    "    # combine the actual target values and predicted values into a single DataFrame for comparison\n",
    "    combined = pd.concat([test['Target'], preds], axis = 1)\n",
    "    \n",
    "    # return the DataFrame with the actual and predicted values\n",
    "    return combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b0f378-7daf-4067-ae63-7b890cf70b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for backtesting the model\n",
    "# every trading year ~ 250 days, thus start of 2500 would be ~ 10 years of data; i.e. starts ~ 10 years back => use 10 years of data to train FIRST model\n",
    "# step is 250 which is ~ 1 year, ergo train the model for one year.  So, start by using 10 years data to train the 11th year, then 11 years of data to train the 12th year, etc...\n",
    "\n",
    "def backtest(data: pd.DataFrame, model: Any, predictors: List[str], start: int = 2500, step: int = 250) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Backtests a machine learning model using rolling time windows.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data: pd.DataFrame\n",
    "        DataFrame containing stock prices and other features.\n",
    "    model: Any\n",
    "        Machine learning model to use.\n",
    "    predictors: List[str]\n",
    "        List of features to use for making predictions.\n",
    "    start: int, optional\n",
    "        Index of the first row to start the backtest (default is 2500).\n",
    "    step: int, optional\n",
    "        Number of rows to include in each test set (default is 250).\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        A concatenated DataFrame of all the predictions.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create an empty list to store all the predictions\n",
    "    all_predictions = []\n",
    "    \n",
    "    # Iterate over the specified range, starting from 'start' and stepping by 'step'; i.e. iterate through data year by year\n",
    "    for i in range(start, data.shape[0], step):\n",
    "        \n",
    "        # Split the data into training and test sets\n",
    "        train = data.iloc[0:i].copy() # The training set includes all rows up to the current index - i.e. all of years preceding current year\n",
    "        test =  data.iloc[i:(i+step)].copy() # The test set includes the next 'step' rows - i.e. current year\n",
    "        \n",
    "        # Use the predict function to make predictions for the test set\n",
    "        predictions = predict(train, test, predictors, model)\n",
    "        \n",
    "        # Append the predictions to the list of all predictions\n",
    "        all_predictions.append(predictions)\n",
    "    \n",
    "    # Concatenate all the predictions into a single DataFrame and return it\n",
    "    return pd.concat(all_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6581a7b9-44c9-4200-acb2-2ca94cbe6cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backtesting the model with sp500 DataFrame, the model we trained earlier, and the list of predictors\n",
    "# the default start value of 2500 means that we start backtesting using data from 10 years ago\n",
    "# the default step value of 250 means that we backtest using one year of data at a time\n",
    "predictions = backtest(sp500, model, predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb417741-030f-4e8c-88ea-5ee915402ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine how many times predited market would go up and how many times predicted market would go down\n",
    "predictions['Predictions'].value_counts()  # '.value_counts()' will provide a numeric count for how many of each type of prediction was made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6923899a-39a5-438c-8e92-d3289dea71ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d564a655-4dd9-4ec6-a538-cf9f00227ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the precision score using predicted values 'preds' and actual values in the test set 'test['Target']'\n",
    "precision_score_2 = precision_score(predictions['Target'], predictions['Predictions'])\n",
    "\n",
    "# visualize precision score\n",
    "print(precision_score_2)\n",
    "\n",
    "# Calculating the balanced accuracy score using predicted values 'preds' and actual values in the test set 'test['Target']'\n",
    "balanced_accuracy_score_2 = balanced_accuracy_score(predictions['Target'], predictions['Predictions'])\n",
    "\n",
    "# visualize balanced accuracy score\n",
    "print(balanced_accuracy_score_2)\n",
    "\n",
    "# Calculating the confusion matrix using predicted values 'preds' and actual values in the test set 'test['Target']'\n",
    "confusion_matrix_2 = confusion_matrix(predictions['Target'], predictions['Predictions'])\n",
    "\n",
    "# visualize confusion matrix\n",
    "print(confusion_matrix_2)\n",
    "\n",
    "# Calculating the classification report using predicted values 'preds' and actual values in the test set 'test['Target']'\n",
    "classification_report_2 = classification_report(predictions['Target'], predictions['Predictions'])\n",
    "\n",
    "# Printing the classification report\n",
    "print(classification_report_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6552155f-7742-4f34-8bf7-7273fdd5e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of days market actually went up\n",
    "predictions['Target'].value_counts() / predictions.shape[0] # value counts of the target divided by the total number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6be8453-4e49-4b97-9cd2-49b1ee8152fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note above, actual performance of market was better than what we predicted; i.e. market rose ~53% and Machine Learning model predicted ~52%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd18b16a-458f-445c-9b06-60d1d1ea1052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# expanding our predictors to use in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be84c25-3d30-4969-98c9-e75aadc06fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of horizons in days that will be used to calculate rolling averages of the S&P 500 stock index.\n",
    "# The list includes values of 2, 5, 60, 250, and 1000, which correspond to different periods of time: two days, one trading week (5 days), \n",
    "# three trading months (60 days), one trading year (250 days), and four trading years (1000 days).\n",
    "horizons = [2, 5, 60, 250, 1000]\n",
    "\n",
    "# Create an empty list to store the new predictor columns to be added to the dataset.\n",
    "new_predictors = []\n",
    "\n",
    "# Loop through each horizon value in the horizons list.\n",
    "for horizon in horizons:\n",
    "    # Calculate the rolling average of the S&P 500 stock index for the current horizon value.\n",
    "    rolling_averages = sp500.rolling(horizon).mean()\n",
    "    \n",
    "    # Create a new column name for the ratio of the closing price to the rolling average for the current horizon value.\n",
    "    ratio_column = f'Close_Ratio_{horizon}'\n",
    "    \n",
    "    # Calculate the ratio of the closing price to the rolling average for the current horizon value, \n",
    "    # and add it as a new column to the S&P 500 dataset.\n",
    "    sp500[ratio_column] = sp500['Close'] / rolling_averages['Close']\n",
    "    \n",
    "    # Create a new column name for the trend over the current horizon value.\n",
    "    trend_column = f'Trend_{horizon}'\n",
    "    \n",
    "    # Calculate the trend over the current horizon value by taking the sum of the 'Target' column over the previous horizon period \n",
    "    # and add it as a new column to the S&P 500 dataset.\n",
    "    # The shift(1) method shifts the data by 1 period so that the sum will be taken over the previous horizon period.\n",
    "    sp500[trend_column] = sp500.shift(1).rolling(horizon).sum()['Target']\n",
    "    \n",
    "    # Add the new ratio and trend column names to the new_predictors list.\n",
    "    new_predictors += [ratio_column, trend_column]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd49675-a0e8-4ac0-8c67-91d17965c40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any rows in the `sp500` DataFrame that contain missing values (i.e., NaN values)\n",
    "sp500 = sp500.dropna()\n",
    "\n",
    "# Visualize the modified `sp500` DataFrame that has had missing value rows removed. \n",
    "# This allows us to inspect the remaining data and ensure that it is complete and appropriate for analysis.\n",
    "sp500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169f7afa-b8ad-4d47-9109-b51a96f96a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model improvement and optimizing - second model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3675dc22-0d8d-4d86-868e-5d8f44c15663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-initialize our model (change model parameters)\n",
    "# set random state, i.e. seed to specific value to ensure repeatability given specified start point of randomness\n",
    "# n_estimators specify the number of random decision trees will use; generally accuracy improves with higher number of decision trees\n",
    "# min_samples_split aids in further preventing 'overfitting'; the higher this value the lower the accuracy, however the higher value will decrease risk of overfit\n",
    "\n",
    "seed = 42\n",
    "model = RandomForestClassifier(n_estimators=200, min_samples_split = 50, random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891e8d89-a142-44f4-a927-6fa76403a83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update predict function; the prediction model\n",
    "\n",
    "def predict(train: pd.DataFrame, test: pd.DataFrame, predictors: List[str], model: Any) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Train the specified model using the specified training data and predictor variables, and use the model to predict\n",
    "    the values of the 'Target' variable for the specified test data.\n",
    "    \n",
    "    Parameters:\n",
    "    train (pd.DataFrame): The training data used to fit the model.\n",
    "    test (pd.DataFrame): The test data used to generate predictions.\n",
    "    predictors (List[str]): A list of column names to use as predictor variables.\n",
    "    model (Any): The model to train and use for predictions.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame containing the actual and predicted values of the 'Target' variable.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Train the model using the specified 'predictors' columns, then attempt to predict the 'Target'\n",
    "    model.fit(train[predictors], train['Target'])\n",
    "    \n",
    "    # Generate predictions with the model using the '.predict()' method passing in 'test' set with the predictors.\n",
    "    # Predictions are probabilities for the class with the highest probability.\n",
    "    # Extract the probabilities for the positive class, which is column 1 of the predictions array.\n",
    "    preds = model.predict_proba(test[predictors])[:,1]\n",
    "    \n",
    "    # Convert predicted probabilities to binary predictions:\n",
    "    # Set predictions >= 0.6 to 1, indicating an expected price increase\n",
    "    # Set predictions < 0.6 to 0, indicating an expected price decrease or no change\n",
    "    # these values will reduce number of actual trding days, thereby increase the chance or probability that day of trade price goes up\n",
    "    preds[preds >= 0.6] = 1\n",
    "    preds[preds < 0.6] = 0\n",
    "    \n",
    "    # Turn the numpy array into a pandas series and name it 'Predictions'\n",
    "    preds = pd.Series(preds, index = test.index, name = 'Predictions') \n",
    "    \n",
    "    # Generate a dataframe of actual values (i.e. test['Target']) and predicted values ('preds') concatenation\n",
    "    combined = pd.concat([test['Target'], preds], axis = 1)\n",
    "    \n",
    "    return combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afca30a-bc09-4b58-80d0-6ae03ea74c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backtesting the S&P 500 data with the earlier created model and asociated predictors created earlier.\n",
    "predictions = backtest(sp500, model, new_predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20241d4-f360-47a0-94d9-e7b6294a30d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions['Predictions'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dcf932-faa9-417a-989f-5d4b3552b156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the precision score using predicted values 'preds' and actual values in the test set 'test['Target']'\n",
    "precision_score_3 = precision_score(predictions['Target'], predictions['Predictions'])\n",
    "\n",
    "# visualize precision score\n",
    "print(precision_score_3)\n",
    "\n",
    "# Calculating the balanced accuracy score using predicted values 'preds' and actual values in the test set 'test['Target']'\n",
    "balanced_accuracy_score_3 = balanced_accuracy_score(predictions['Target'], predictions['Predictions'])\n",
    "\n",
    "# visualize balanced accuracy score\n",
    "print(balanced_accuracy_score_3)\n",
    "\n",
    "# Calculating the confusion matrix using predicted values 'preds' and actual values in the test set 'test['Target']'\n",
    "confusion_matrix_3 = confusion_matrix(predictions['Target'], predictions['Predictions'])\n",
    "\n",
    "# visualize confusion matrix\n",
    "print(confusion_matrix_3)\n",
    "\n",
    "# Calculating the classification report using predicted values 'preds' and actual values in the test set 'test['Target']'\n",
    "classification_report_3 = classification_report(predictions['Target'], predictions['Predictions'])\n",
    "\n",
    "# Printing the classification report\n",
    "print(classification_report_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb45c80-92f0-4d5b-a065-b30e08479529",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
